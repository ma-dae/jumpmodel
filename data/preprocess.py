import pandas as pd
import numpy as np
from pathlib import Path

# ðŸ“ ê²½ë¡œ ì„¤ì •
RAW_DATA_DIR = Path("data/raw")
CLEAN_DATA_DIR = Path("data/clean")
CLEAN_DATA_DIR.mkdir(parents=True, exist_ok=True)

def preprocess_for_gupdeung(df: pd.DataFrame, ticker: str = "") -> pd.DataFrame:
    """
    ðŸ“Œ ê¸‰ë“±ì£¼ íƒì§€ ëª¨ë¸ìš© ì „ì²˜ë¦¬ í•¨ìˆ˜
    """

    required_cols = ["Date", "Open", "High", "Low", "Raw_Close", "Adj_Close", "Volume"]
    missing = [col for col in required_cols if col not in df.columns]
    if missing:
        raise ValueError(f"[âŒ] {ticker}: ëˆ„ë½ëœ ì»¬ëŸ¼ {missing}")

    # ì´ˆê¸° ë³µì‚¬ë³¸ ìƒì„± (ê²½ê³  ë°©ì§€)
    df = df.copy()

    # ë‚ ì§œ ì •ì œ + ë³€í™˜
    df["Date"] = df["Date"].astype(str).str.strip()
    df = df[df["Date"].str.match(r"^\d{4}-\d{2}-\d{2}$", na=False)]
    df["Date"] = pd.to_datetime(df["Date"], format="%Y-%m-%d", errors="coerce")

    # ìˆ˜ì¹˜í˜• ë³€í™˜
    for col in required_cols[1:]:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    # ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
    df = df.sort_values("Date").drop_duplicates(subset="Date")

    # ê±°ëž˜ëŸ‰ 0 ì œê±°
    df = df[df["Volume"] > 0]

    # ê°€ê²© ìŒìˆ˜ ì œê±°
    for col in ["Open", "High", "Low", "Raw_Close", "Adj_Close"]:
        df = df[df[col] > 0]

    # OHLC ë…¼ë¦¬ ê²€ì¦
    df = df[
        (df["High"] >= df["Low"]) &
        (df["High"] >= df["Open"]) &
        (df["High"] >= df["Raw_Close"]) &
        (df["Low"] <= df["Open"]) &
        (df["Low"] <= df["Raw_Close"])
    ]

    # í•„í„°ë§ í›„ ìƒˆ ë³µì‚¬ë³¸ ìƒì„± (ì¤‘ìš”!)
    df = df.copy()

    # ìˆ˜ìµë¥  ê³„ì‚°
    df["Return"] = df["Adj_Close"].pct_change()
    df["LogReturn"] = np.log(df["Adj_Close"] / df["Adj_Close"].shift(1))

    # ë³€ë™ì„± ì§€í‘œ
    df["Volatility"] = df["High"] - df["Low"]

    # RSI ê³„ì‚° (14ì¼ ê¸°ì¤€)
    delta = df["Adj_Close"].diff()
    gain = np.where(delta > 0, delta, 0)
    loss = np.where(delta < 0, -delta, 0)
    avg_gain = pd.Series(gain).rolling(window=14).mean()
    avg_loss = pd.Series(loss).rolling(window=14).mean()
    
    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    rs = np.where(avg_loss != 0, avg_gain / avg_loss, 0)
    df["RSI_14"] = 100 - (100 / (1 + rs))

    # ê²°ì¸¡ì¹˜ ë° ë¬´í•œê°’ ì œê±°
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.dropna(subset=["Return", "LogReturn", "RSI_14"])

    return df

def process_ticker(ticker: str):
    """ê°œë³„ í‹°ì»¤ ì²˜ë¦¬"""
    path = RAW_DATA_DIR / f"{ticker}.csv"
    if not path.exists():
        print(f"[âŒ] {ticker}: íŒŒì¼ ì—†ìŒ")
        return False

    try:
        df_raw = pd.read_csv(path)
        df_clean = preprocess_for_gupdeung(df_raw, ticker)

        if len(df_clean) < 100:
            print(f"[âš ï¸] {ticker}: ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŒ ({len(df_clean)}í–‰)")

        df_clean.to_csv(CLEAN_DATA_DIR / f"{ticker}.csv", index=False)
        print(f"[âœ…] {ticker}: ì „ì²˜ë¦¬ ì™„ë£Œ â†’ {len(df_clean)}í–‰")
        return True
        
    except Exception as e:
        print(f"[ðŸ’¥] {ticker}: ì „ì²˜ë¦¬ ì‹¤íŒ¨ â†’ {e}")
        return False

def process_all():
    """ëª¨ë“  íŒŒì¼ ì²˜ë¦¬"""
    files = list(RAW_DATA_DIR.glob("*.csv"))
    success_count = 0
    
    for file in files:
        ticker = file.stem
        if process_ticker(ticker):
            success_count += 1
    
    print(f"\n[ðŸ“Š] ì „ì²´ ê²°ê³¼: {success_count}/{len(files)} ì„±ê³µ")

if __name__ == "__main__":
    process_all()